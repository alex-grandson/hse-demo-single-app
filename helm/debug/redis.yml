---
# Source: hse-app/templates/crashloopbackoff-inspect.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: podhealer
  namespace: default
---
# Source: hse-app/templates/terminating-status-inspect.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: podhealer
  namespace: default
---
# Source: hse-app/templates/crashloopbackoff-inspect.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: podhealer
  namespace: default
rules:
  - apiGroups:
      - ''
    resources:
      - pods
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - apps
    resources:
      - deployments
    verbs:
      - create
      - get
      - delete
      - list
      - patch
      - update
      - watch
---
# Source: hse-app/templates/terminating-status-inspect.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: podhealer
  namespace: default
rules:
  - apiGroups:
      - ''
    resources:
      - pods
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - apps
    resources:
      - deployments
    verbs:
      - create
      - get
      - delete
      - list
      - patch
      - update
      - watch
---
# Source: hse-app/templates/crashloopbackoff-inspect.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: podhealer
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: podhealer
subjects:
  - kind: ServiceAccount
    name: podhealer
    namespace: default
---
# Source: hse-app/templates/terminating-status-inspect.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: podhealer
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: podhealer
subjects:
  - kind: ServiceAccount
    name: podhealer
    namespace: default
---
# Source: hse-app/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-db
  namespace: default
  labels:
    app: redis-db
    name: redis-db
spec:
  ports:
    - name: 6379-tcp
      port: 6379
      protocol: TCP
      targetPort: 6379
  selector:
    app.kubernetes.io/component: redis-db
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
---
# Source: hse-app/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-db
  namespace: default
  labels:
    
    "app.kubernetes.io/managed-by": "Helm"
    "app.kubernetes.io/instance": "redis"
    "app.kubernetes.io/component": "redis-db"
    "app": "redis-db"
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      {}
  selector:
    matchLabels:
      "app.kubernetes.io/component": "redis-db"
      "app.kubernetes.io/instance": "redis"
  revisionHistoryLimit: 3
  template:
    metadata:
      labels:
        
        "app.kubernetes.io/managed-by": "Helm"
        "app.kubernetes.io/instance": "redis"
        "app.kubernetes.io/component": "redis-db"
        "app": "redis-db"
    spec:
      
      
      
      containers:
        - name: redis-db
          image: "bitnami/redis:6.2.16-debian-12-r3"
          imagePullPolicy: Always
          
          env:
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
          ports:
            - name: 6379-tcp
              containerPort: 6379
          
          
          livenessProbe:
            exec:
              command:
              - redis-server
              - --version
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
              - redis-server
              - --version
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
          startupProbe:
            exec:
              command:
              - redis-server
              - --version
            initialDelaySeconds: 90
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 5
          resources:
            limits:
              cpu: 300m
              memory: 1200Mi
            requests:
              cpu: 300m
              memory: 600Mi
---
# Source: hse-app/templates/crashloopbackoff-inspect.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: inspect-crash-pod-15min
  namespace: default
spec:
  successfulJobsHistoryLimit: 100
  failedJobsHistoryLimit: 100
  concurrencyPolicy: Forbid
  schedule: "*/15 * * * *"
  jobTemplate:
    spec:
      backoffLimit: 5
      activeDeadlineSeconds: 500
      ttlSecondsAfterFinished: 500
      template:
        spec:
          serviceAccountName: podhealer
          restartPolicy: Never
          containers:
            - name: 'kubectl'
              image: 'artifactory-spb.atmosphere.yadro.com/yadro-proxy-dockerhub/bitnami/kubectl:latest'
              command:
                - bash
                - -c
                - >-
                  for p in $(kubectl get pods | grep -i crashloopbackoff | awk '{print $1}' | cut -d'-' -f-2); do kubectl rollout restart deployment/$p && kubectl rollout status deployment/$p;done
---
# Source: hse-app/templates/terminating-status-inspect.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: inspect-suspending-pods-40min
  namespace: default
spec:
  successfulJobsHistoryLimit: 100
  failedJobsHistoryLimit: 100
  concurrencyPolicy: Forbid
  schedule: "*/40 * * * *"
  jobTemplate:
    spec:
      backoffLimit: 5
      activeDeadlineSeconds: 500
      ttlSecondsAfterFinished: 500
      template:
        spec:
          serviceAccountName: podhealer
          restartPolicy: Never
          containers:
            - name: 'kubectl'
              image: 'artifactory-spb.atmosphere.yadro.com/yadro-proxy-dockerhub/bitnami/kubectl:latest'
              command:
                - bash
                - -c
                - >-
                  for p in $(kubectl get pods | grep -i terminating | awk '{print $1}'); do kubectl delete pod $p --grace-period=0 --force;done
